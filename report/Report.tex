\documentclass{article}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{graphicx}
\title{\textbf{ECE 6560 Final Project - Image Smoothing}}
\author{Alexander Domagala}
\date{Spring 2024}

\begin{document}
  \maketitle

  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  % Problem Description
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \section{Problem Description}
  Although image processing techniques began to appear around the 1960s, the proliferation of inexpensive
  digital computing power has greatly widened the application pool. Image processing techniques
  can now be found in areas such as digital photography, medical imaging, and object detection/tracking.\\

  \noindent
  All these applicationss can be affected by a very common problem: image noise.
  Noise can render further processing ineffective, thus there exist a variety of approaches by which
  we can attempt to smooth and denoise images. Traditional image processing techniques
  may rely on fixed kernels that can be swept over an image in order to smooth it. However,
  this type of approach can result in uniform smoothing that blurs the edges of an image.\\

  \noindent
  We can instead leverage PDEs to describe how an image should be updated depending on its local characteristics.
  More specifically, we will examine Ansiotropic Diffusion. This technique can be used to reduce image noise
  while lessening the blurring that is done to edges.\\

  \noindent
  Linear diffusion (FIX PUT TV DIFFUSION AND PAPER LINK) filtering as described in \cite{Thomas_book} (update citation!!!) is a simple,
  yet effective PDE method for image smoothing. The method has a linear energy functional. This means that at each
  point in the image, the rate of diffusion is linear. Relatively smooth areas that contain some noise will have
  the noise lessened, while strong edges will only slowly diffuse (as opposed to a quadratic penalty which would
  yield a great amount of diffusion).\\

  \noindent
  Instead of continuing the diffusion at any gradient, this project will explore what happens if we
  attempt to have a near zero amount of diffusion for the higher gradients (where we have edges).


  \noindent
  https://www.mia.uni-saarland.de/weickert/Papers/book.pdf\\



  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  % Mathematical Modeling
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \newpage
  \section{Mathematical Modeling}
  \subsection{Introduction}
  Before explicitly developing our PDE, we must first understand the behavior that we want our system to have.
  The Calculus of Variations is an extremely powerful tool that will help us arrive at a PDE that reflects
  the desired behavior. Examine the following equation
  \begin{center}
    \begin{tabular}{l}
      $E(I) = \int_{0}^{1} \int_{0}^{1} L(I, I_{x}, I_{y}, x, y) dx dy$
    \end{tabular}
  \end{center}

  \noindent
  $E(I)$ is the energy functional. Similar to the common understanding of a function, it can output
  a single value. The key difference is that the input to the energy functional, is another entire
  function that will depend on traditional variables like $x,y$, etc.\\

  \noindent
  Note that all notation before section (4) is for continuous space: $x,y \in \mathbb{R}$. Thus,
  the input to the energy functional can be thought of as an image with an infinite amount of pixels: $I(x,y)$.\\

  \noindent
  During class, the following example was introduced. If we want to measure how noisy a certain image is,
  we could measure the average value of the image's gradient squared.
  \begin{center}
    \begin{tabular}{l}
      $E(I) = \int_{0}^{1} \int_{0}^{1} \frac{1}{2} (\| \nabla_{I} \|)^{2} dx$
    \end{tabular}
  \end{center}

  \noindent
  Noiser images will have larger values for $E(I)$ while smoother images will have a smaller values for $E(I)$.
  Let's examine how the design of an energy functional impacts image smoothing.\\

  \noindent
  In traditional calculus, if given some function, we can obtain its derivative and find a value that minimizes the function.
  In the Calculus of Variations, if given some energy functional, we can obtain a PDE (via gradient descent) that 
  sets constraints so that a function can minimize the energy functional.\\

  \newpage
  \noindent
  In the case of the example, minimization yields the Linear Heat Equation
  \begin{center}
    $I_{t} = \Delta I$
  \end{center}
  The Linear Heat Equation optimally reduces the average squared value of the gradient in an image.
  This means that areas of high gradient in the image will be aggresively smoothed. Furthermore, as the
  magnitude of the gradient increases, the smoothing effect grows quadratically. Hence, the edges (which
  are areas of higher gradient) are not very well preserved with this technique.
  \begin{center}
    Quadratic Penalty: $x^2$\\
    \vspace{12pt}
    \includegraphics[scale=0.5]{../report_images/squared.png}
  \end{center}
  \vspace{12pt}

  \noindent
  A major improvement is to instead use a linear penalty. Smoothing that takes place at the edges
  will be less extreme than the previous case. This means that enough diffusion could take place
  in areas of low gradient such that the image appears to be denoised. Then, the diffusion can be stopped.
  Although the edges also experience diffusion, the penalty is much more lenient than the previous case.
  This method is known as Total Variation Diffusion.
  \begin{center}
    Linear Penalty: $x$\\
    \vspace{12pt}
    \includegraphics[scale=0.5]{../report_images/linear.png}
  \end{center}


  \newpage
  \noindent
  TODO:\\
    INTRODUCE Ansiotropic Diffusion\\
    Describe the NEW energy functional\\
  $\| \nabla_{I} \|$

  \begin{center}
    Sigmoidal Penalty: $ADD$\\
    \vspace{12pt}
    \includegraphics[scale=0.5]{../report_images/sigmoid.png}
  \end{center}
  \vspace{12pt}

  \noindent


    

  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  % Derivation of PDE
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \newpage
  \section{Derivation of PDE}

    \noindent Now, let's introduce the Euler-Lagrange equation
      \begin{center}
        \begin{tabular}{l}
          $L_{f} - \frac{\partial}{\partial x}L_{f'} = 0$ (1-D)\\
          $L_{I} - \frac{\partial}{\partial x}L_{I_{x}} - \frac{\partial}{\partial y}L_{I_{y}} = 0$ (2-D)\\
        \end{tabular}
      \end{center}
    \vspace{12pt}

    \noindent
    We can begin working towards obtaining our PDE by setting up a gradient descent
      \begin{center}
        \begin{tabular}{l}
          $I_{t} = -\nabla_{I}E$\\
          $I_{t} = -L_{I} + \frac{\partial}{\partial x}L_{I_{x}} + \frac{\partial}{\partial y}L_{I_{y}}$
        \end{tabular}
      \end{center}
    \vspace{12pt}

    \noindent
    We will now have to compute terms $L_{I}$, $L_{I_{x}}$, $L_{I_{y}}$ using the previously obtained energy functional
      \begin{center}
        \begin{tabular}{l}
          $L(I,I_{x},I_{y},x,y) = \frac{\lambda}{1+e^{\alpha}}$, where $\alpha = -\frac{1}{\beta}(\| \nabla_{I} \|)$\\
          We will also include a term $\epsilon$ such that $\| \nabla_{I} \| = \sqrt{I_{x}^2 + I_{y}^2 + \epsilon^2}$\\
          $\epsilon$ is a constant used to prevent stability issues and will be examined more\\
          \vspace{12pt}
          closely in section (4).\\
          $L_{I} = \frac{\partial}{\partial I}(L)$\\
          $L_{I} = 0$\\
          $L_{I_{x}} = \frac{\partial}{\partial I_{x}}(L)$\\
          $L_{I_{x}} = \frac{\lambda}{\beta} \frac{e^\alpha}{(1+e^{\alpha})^2} \frac{I_{x}}{\sqrt{I_{x}^2 + I_{y}^2 + \epsilon^2}}$\\
          $L_{I_{y}} = \frac{\partial}{\partial I_{y}}(L)$\\
          $L_{I_{y}} = \frac{\lambda}{\beta} \frac{e^\alpha}{(1+e^{\alpha})^2} \frac{I_{y}}{\sqrt{I_{x}^2 + I_{y}^2 + \epsilon^2}}$\\
        \end{tabular}
      \end{center}
      \vspace{12pt}

    \noindent
    Now that we have obtained our expressions for $L_{I_{x}}$ and $L_{I_{y}}$, we must compute their partial derivatives
    as shown by the Euler-Lagrange equation. This will be shown for $\frac{\partial}{\partial x}L_{I_{x}}$. $\frac{\partial}{\partial y}L_{I_{y}}$ will be obtained by examining
    the expression of $\frac{\partial}{\partial x}L_{I_{x}}$.\\

    \noindent
    Let $\phi$ denote $\frac{e^\alpha}{(1+e^{\alpha})^2}$ and let $\gamma$ denote $\frac{I_{x}}{\sqrt{I_{x}^2 + I_{y}^2 + \epsilon^2}}$.
    We can begin finding $\frac{\partial}{\partial x}L_{I_{x}}$ by using the product-rule $\frac{\partial}{\partial x}(\gamma)\phi + \frac{\partial}{\partial x}(\phi)\gamma$.
    We will start with the left side of the sum. Note that we must include $\frac{\lambda}{\beta}$ in the final expression.\\
    \begin{center}
      \begin{tabular}{l}
        \vspace{12pt}
        $\frac{\partial}{\partial x}(\gamma)\phi$\\
        \vspace{12pt}
        $\frac{\partial}{\partial x}(\frac{I_{x}}{\sqrt{I_{x}^2 + I_{y}^2 + \epsilon^2}})\phi$\\
        $(\frac{I_{xx}}{(I_{x}^2 + I_{y}^2 + \epsilon^2)^\frac{1}{2}} + \frac{I_{x}}{(I_{x}^2 + I_{y}^2 + \epsilon^2)^\frac{3}{2}}(I_{x}I_{xx} + I_{y}I_{xy}))\phi$
      \end{tabular}
    \end{center}
    \vspace{12pt}
    
    \newpage
    \noindent
    We can now examine the right side of $\frac{\partial}{\partial x}(\gamma)\phi + \frac{\partial}{\partial x}(\phi)\gamma$.
    \begin{center}
      \begin{tabular}{l}
        $\frac{\partial}{\partial x}(\phi)\gamma$\\
        $\frac{\partial}{\partial x}(\frac{e^\alpha}{(1+e^{\alpha})^2})\gamma$\\
        $\frac{\partial}{\partial x}((e^\alpha)(1+e^{\alpha})^{-2})\gamma$\\
      \end{tabular}
    \end{center}

    \noindent
    We see that we will need to again perform the product-rule between $(e^\alpha)$ and\\
    $(1+e^{\alpha})^{-2}$. Taking the partial derivative of $(e^\alpha)$
    \begin{center}
      \begin{tabular}{l}
        $-\frac{1}{\beta} e^{\alpha} \frac{1}{(I_{x}^2 + I_{y}^2 + \epsilon^2)^\frac{1}{2}} (I_{x}I_{xx}+I_{y}I_{xy})$
      \end{tabular}
    \end{center}

    \noindent
    Taking the partial derivative of $(1+e^{\alpha})^{-2}$
    \begin{center}
      \begin{tabular}{l}
        $-2(1+e^{\alpha})^{-3} (-\frac{1}{\beta} e^{\alpha} \frac{1}{(I_{x}^2 + I_{y}^2 + \epsilon^2)^\frac{1}{2}} (I_{x}I_{xx}+I_{y}I_{xy})) $
      \end{tabular}
    \end{center}

    \noindent
    Thus, after factoring common terms, $\frac{\partial}{\partial x}((e^\alpha)(1+e^{\alpha})^{-2})$ yields
    \begin{center}
      \begin{tabular}{l}
        $[-\frac{1}{\beta} (e^\alpha) (\frac{1}{(I_{x}^2 + I_{y}^2 + \epsilon^2)^\frac{1}{2}}) (I_{x}I_{xx}+I_{y}I_{xy})] [(1+e^{\alpha})^{-2} + (e^\alpha)(-2(1+e^{\alpha})^{-3})]$\\
      \end{tabular}
    \end{center}
    \vspace{12pt}
    \vspace{12pt}

  \noindent
  We have reached the final expression for $\frac{\partial}{\partial x}L_{I_{x}}$
  \begin{center}
    \begin{tabular}{l}
      \vspace{12pt}
      $\frac{\partial}{\partial x}L_{I_{x}} =$\\
      \vspace{12pt}
      $\frac{\lambda}{\beta}[ (\frac{I_{xx}}{(I_{x}^2 + I_{y}^2 + \epsilon^2)^\frac{1}{2}}) - (\frac{I_{x}}{(I_{x}^2 + I_{y}^2 + \epsilon^2)^\frac{3}{2}}) (I_{x}I_{xx} + I_{y}I_{xy}) (\frac{e^\alpha}{(1+e^{\alpha})^2}) +$\\
      \vspace{12pt}
      $(\frac{I_{x}}{(I_{x}^2 + I_{y}^2 + \epsilon^2)^\frac{1}{2}}) [-\frac{1}{\beta} (e^\alpha) (\frac{1}{(I_{x}^2 + I_{y}^2 + \epsilon^2)^\frac{1}{2}}) (I_{x}I_{xx}+I_{y}I_{xy})] [(1+e^{\alpha})^{-2} + (e^\alpha)(-2(1+e^{\alpha})^{-3})]]$
    \end{tabular}
  \end{center}
  \vspace{12pt}

  \noindent
    $\frac{\partial}{\partial y}L_{I_{y}}$ can be obtained by modifying $\frac{\partial}{\partial x}L_{I_{x}}$
    \begin{center}
      \begin{tabular}{l}
        \vspace{12pt}
        $\frac{\partial}{\partial y}L_{I_{y}} =$\\
        \vspace{12pt}
        $\frac{\lambda}{\beta}[ (\frac{I_{yy}}{(I_{x}^2 + I_{y}^2 + \epsilon^2)^\frac{1}{2}}) - (\frac{I_{y}}{(I_{x}^2 + I_{y}^2 + \epsilon^2)^\frac{3}{2}}) (I_{x}I_{xy} + I_{y}I_{yy}) (\frac{e^\alpha}{(1+e^{\alpha})^2}) +$\\
        \vspace{12pt}
        $(\frac{I_{y}}{(I_{x}^2 + I_{y}^2 + \epsilon^2)^\frac{1}{2}}) [-\frac{1}{\beta} (e^\alpha) (\frac{1}{(I_{x}^2 + I_{y}^2 + \epsilon^2)^\frac{1}{2}}) (I_{x}I_{xy}+I_{y}I_{yy})] [(1+e^{\alpha})^{-2} + (e^\alpha)(-2(1+e^{\alpha})^{-3})]]$
      \end{tabular}
    \end{center}
    \vspace{12pt}

    \newpage
    \noindent
    Our final gradient-descent PDE is
    \begin{center}
      \begin{tabular}{l}
        \vspace{12pt}
        $I_{t} = \frac{\lambda}{\beta}[ (\frac{I_{xx}}{(I_{x}^2 + I_{y}^2 + \epsilon^2)^\frac{1}{2}}) - (\frac{I_{x}}{(I_{x}^2 + I_{y}^2 + \epsilon^2)^\frac{3}{2}}) (I_{x}I_{xx} + I_{y}I_{xy}) (\frac{e^\alpha}{(1+e^{\alpha})^2}) +$\\
        \vspace{12pt}
        $(\frac{I_{x}}{(I_{x}^2 + I_{y}^2 + \epsilon^2)^\frac{1}{2}}) [-\frac{1}{\beta} (e^\alpha) (\frac{1}{(I_{x}^2 + I_{y}^2 + \epsilon^2)^\frac{1}{2}}) (I_{x}I_{xx}+I_{y}I_{xy})] [(1+e^{\alpha})^{-2} + (e^\alpha)(-2(1+e^{\alpha})^{-3})]+ $\\
        \vspace{12pt}
        $(\frac{I_{yy}}{(I_{x}^2 + I_{y}^2 + \epsilon^2)^\frac{1}{2}}) - (\frac{I_{y}}{(I_{x}^2 + I_{y}^2 + \epsilon^2)^\frac{3}{2}}) (I_{x}I_{xy} + I_{y}I_{yy}) (\frac{e^\alpha}{(1+e^{\alpha})^2}) +$\\
        \vspace{12pt}
        $(\frac{I_{y}}{(I_{x}^2 + I_{y}^2 + \epsilon^2)^\frac{1}{2}}) [-\frac{1}{\beta} (e^\alpha) (\frac{1}{(I_{x}^2 + I_{y}^2 + \epsilon^2)^\frac{1}{2}}) (I_{x}I_{xy}+I_{y}I_{yy})] [(1+e^{\alpha})^{-2} + (e^\alpha)(-2(1+e^{\alpha})^{-3})]]$
        \vspace{12pt}
      \end{tabular}
    \end{center}

    \noindent
    Where $\alpha = -\frac{1}{\beta}(\| \nabla_{I} \|)$ and $\| \nabla_{I} \| = \sqrt{I_{x}^2 + I_{y}^2 + \epsilon^2}$\\



  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  % Discretization and Implementation
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \newpage
  \section{Discretization and Implementation}
  \noindent
  The PDE that was obtained in the previous section is only applicable for continuous
  time and space variables. We must discretize the PDE so that it can actually be implemented in software.\\

  \subsection{Numerical Differentiation}
  \noindent
  Before explicitly discretizing the PDE, we must also obtain numeric approximations for its partial
  derivatives. Beginning with the left side of the PDE, let's approximate $I_{t}$. 
  It is important to understand that the PDE is defining how the image will be smoothed
  over successive iterations. In other words, it specifies how the image is updated as time increases.
  Thus, it is appropriate to use a forward difference.\\

  \noindent
  The forward difference for a single (space) variable can be obtained using the 
  numeric differentiation method shown in class
  \begin{center}
    \begin{tabular}{l}
      \vspace{12pt}
      $f(x,t) = f(x,t)$\\
      $f(x,t+\Delta t) = f(x,t) + \Delta t f'(x,t) + O(\Delta t^2)$\\
    \end{tabular}
  \end{center}

  \noindent
  After eliminating $f(x,t)$ terms on the right side of the above equations,
  we obtain our approximation. Note that the approximation includes an error term $O(\Delta t)$.
  \begin{center}
    \begin{tabular}{l}
      $f'(x,t) = \frac{f(x,t+\Delta t) - f(x,t)}{\Delta t}$\\
    \end{tabular}
  \end{center}

  \noindent
  Expanding this process to two (space) dimensions yields
  \begin{center}
    \begin{tabular}{l}
      $I_{t}(x,y,t) = \frac{I(x,y,t+\Delta t) - I(x,y,t)}{\Delta t}$\\
    \end{tabular}
  \end{center}
  \vspace{12pt}

  \noindent
  Continuing with the right side of the PDE, approximations are needed for the following partial
  derivatives: $I_{x}$, $I_{y}$, $I_{xx}$, $I_{yy}$, and $I_{xy}$. Since these approximations
  capture how the image is changing with respect to space, it is more appropriate to use a
  central difference rather than a forward difference.\\
  
  \noindent
  Suppose we are trying to approximate
  $I_{x}$. A forward difference would introduce some bias because the value of $I_{x}$ at
  a certain point is dependent on an adjacent image value in the positive x-direction only.
  The central difference will balance out the value of $I_{x}$ since both directions
  are considered. The intent is to make the approximation more robust to variations in image content. \\

  \newpage
  \noindent
  The central difference for a single (space) variable can be obtained using the
  numeric differentiation method shown in class
  \begin{center}
    \begin{tabular}{l}
      \vspace{12pt}
      $f(x,t) = f(x,t)$\\
      \vspace{12pt}
      $f(x+\Delta x,t) = f(x,t) + \Delta x f'(x,t) + O(\Delta x^2)$\\
      \vspace{12pt}
      $f(x-\Delta x,t) = f(x,t) - \Delta x f'(x,t) + O(\Delta x^2)$\\
    \end{tabular}
  \end{center}

  \noindent
  After eliminating $f(x,t)$ terms on the right side of the above equations,
  we obtain our approximation. Note that the approximation includes an error term $O(\Delta x)$.
  \begin{center}
    \begin{tabular}{l}
      \vspace{12pt}
      $f'(x,t) = \frac{f(x+\Delta x,t) - f(x-\Delta x,t)}{2\Delta x}$\\
    \end{tabular}
  \end{center}

  \noindent
  Expanding this process to two (space) dimensions yields
  \begin{center}
    \begin{tabular}{l}
      \vspace{12pt}
      $I_{x}(x,y,t) = \frac{I(x+\Delta x,y,t) - I(x-\Delta x,y,t)}{2\Delta x}$\\
      \vspace{12pt}
      $I_{y}(x,y,t) = \frac{I(x,y+\Delta y,t) - I(x,y-\Delta y,t)}{2\Delta y}$\\
    \end{tabular}
  \end{center}

  \noindent
  In order to obtain approximations for $I_{xx}$ and $I_{yy}$, we can simply add additional
  terms to the Taylor Series expansion and then repeat the elimination process
  \begin{center}
    \begin{tabular}{l}
      \vspace{12pt}
      $I_{xx}(x,y,t) = \frac{I(x+\Delta x,y,t) - 2I(x,y,t) + I(x-\Delta x,y,t)}{\Delta x^{2}}$\\
      \vspace{12pt}
      $I_{yy}(x,y,t) = \frac{I(x,y+\Delta y,t) - 2I(x,y,t) + I(x,y-\Delta y,t)}{\Delta y^{2}}$\\
    \end{tabular}
  \end{center}
  \vspace{12pt}

  \noindent
  Finally, we need to obtain an approximation for the mixed partial derivative $I_{xy}$.
  This is achieved by taking the central difference approximation for $I_{x}$ and 
  applying another central difference with respect to $y$
  \begin{center}
    \begin{tabular}{l}
      \vspace{12pt}
      $I_{x}(x,y,t) = \frac{I(x+\Delta x,y,t)}{2\Delta x} - \frac{I(x-\Delta x,y,t)}{2\Delta x}$\\
      \vspace{12pt}
      $I_{xy}(x,y,t) = \frac{\partial}{\partial y}(\frac{I(x+\Delta x,y,t)}{2\Delta x}) - \frac{\partial}{\partial y}(\frac{I(x-\Delta x,y,t)}{2\Delta x})$\\
      \vspace{12pt}
      $I_{xy}(x,y,t) = \frac{1}{2\Delta y}(\frac{I(x+\Delta x,y+\Delta y,t)}{2\Delta x} - \frac{I(x+\Delta x,y-\Delta y,t)}{2\Delta x}) - \frac{1}{2\Delta y}(\frac{I(x-\Delta x,y+\Delta y,t)}{2\Delta x} - \frac{I(x-\Delta x,y-\Delta y,t)}{2\Delta x})$\\
      \vspace{12pt}
      $I_{xy}(x,y,t) = \frac{I(x+\Delta x,y+\Delta y,t) - I(x+\Delta x,y-\Delta y,t) - I(x-\Delta x,y+\Delta y,t) + I(x-\Delta x,y-\Delta y,t)}{4 \Delta x\Delta y} $ \\
    \end{tabular}
  \end{center}

  \newpage
  \subsection{Parameter Selection}
  \noindent
  Now that we have obtained all the necessary numeric approximations of the partial derivatives, we
  can move towards defining a scheme that discretizes the PDE. This involves selecting
  some important parameters.\\

  \noindent
  First, we will examine what is an appropriate time-step $\Delta t$. Previously,
  we defined $I_{t}(x,y,t) = \frac{I(x,y,t+\Delta t) - I(x,y,t)}{\Delta t}$. In our implementation,
  $\Delta t$ will multiply the right side of the PDE. The product will then be added to
  the current image to obtain the updated image. Thus, we must find a CFL condition
  so that we can find an appropriate time-step. Recall that during the derivation of
  the PDE, a term $\epsilon$ was included so that $\| \nabla_{I} \| = \sqrt{I_{x}^2 + I_{y}^2 + \epsilon^2}$.
  Suppose the gradient became very small, all terms $I_{x}$ and $I_{y}$ in the PDE would go to zero and 
  we would be left with $I_{t} = \frac{\lambda}{\beta}[(\frac{I_{xx}+I_{yy}}{(\epsilon^2)^\frac{1}{2}})]$.
  Then we simply have the heat equation
  \begin{center}
    \begin{tabular}{l}
      $I_{t} = \frac{\lambda}{\beta \epsilon}\Delta I$
    \end{tabular}
  \end{center}

  \noindent
  We can include our constants with the form of the CFL condition for the two-dimensional linear heat equation
  as shown in class
  \begin{center}
    \begin{tabular}{l}
      \vspace{12pt}
      $\frac{\lambda}{\beta \epsilon} \leq \frac{1}{4} \Delta x^2$\\
      $\Delta t \leq \frac{1}{4} \frac{\beta \epsilon}{\lambda} \Delta x^2$\\
    \end{tabular}
  \end{center}

  \noindent
  With a bound set for  $\Delta t$, we can now shift our focus to $\Delta x$ and $\Delta y$. Since
  a digital image is composed of a finite number of individual pixels, we will compute the different
  partial derivative approximations by simply taking our current pixel and applying an offset of
  +1 or -1 to the current pixel's $x$ and $y$ indices. For example, for $I_{x}$
  \begin{center}
    \begin{tabular}{l}
      $I_{x}(x,y,t) = \frac{I(x+\Delta x,y,t)}{2\Delta x} - \frac{I(x-\Delta x,y,t)}{2\Delta x}$, we can
      \vspace{12pt}
      compute $I_{x}$ at pixel $(a,b)$ as\\
      $I_{x}(a,b,t) = \frac{I(a+1,b,t)}{2(1)} - \frac{I(a-1,b,t)}{2(1)}$
    \end{tabular}
  \end{center}

  \noindent
  Finally, we will iterate through each pixel in the image and compute how the pixel should
  be updated based on the PDE.

  \newpage
  \subsection{Implementation Summary}
  \noindent
  \vspace{12pt}
  \textbf{PDE}\\
  \begin{tabular}{l}
    \vspace{12pt}
    $I_{t} = \frac{\lambda}{\beta}[ (\frac{I_{xx}}{(I_{x}^2 + I_{y}^2 + \epsilon^2)^\frac{1}{2}}) - (\frac{I_{x}}{(I_{x}^2 + I_{y}^2 + \epsilon^2)^\frac{3}{2}}) (I_{x}I_{xx} + I_{y}I_{xy}) (\frac{e^\alpha}{(1+e^{\alpha})^2}) +$\\
    \vspace{12pt}
    $(\frac{I_{x}}{(I_{x}^2 + I_{y}^2 + \epsilon^2)^\frac{1}{2}}) [-\frac{1}{\beta} (e^\alpha) (\frac{1}{(I_{x}^2 + I_{y}^2 + \epsilon^2)^\frac{1}{2}}) (I_{x}I_{xx}+I_{y}I_{xy})] [(1+e^{\alpha})^{-2} + (e^\alpha)(-2(1+e^{\alpha})^{-3})]+ $\\
    \vspace{12pt}
    $(\frac{I_{yy}}{(I_{x}^2 + I_{y}^2 + \epsilon^2)^\frac{1}{2}}) - (\frac{I_{y}}{(I_{x}^2 + I_{y}^2 + \epsilon^2)^\frac{3}{2}}) (I_{x}I_{xy} + I_{y}I_{yy}) (\frac{e^\alpha}{(1+e^{\alpha})^2}) +$\\
    \vspace{12pt}
    $(\frac{I_{y}}{(I_{x}^2 + I_{y}^2 + \epsilon^2)^\frac{1}{2}}) [-\frac{1}{\beta} (e^\alpha) (\frac{1}{(I_{x}^2 + I_{y}^2 + \epsilon^2)^\frac{1}{2}}) (I_{x}I_{xy}+I_{y}I_{yy})] [(1+e^{\alpha})^{-2} + (e^\alpha)(-2(1+e^{\alpha})^{-3})]]$\\
    \vspace{12pt}
    Where $\alpha = -\frac{1}{\beta}(\| \nabla_{I} \|)$ and $\| \nabla_{I} \| = \sqrt{I_{x}^2 + I_{y}^2 + \epsilon^2}$\\
  \end{tabular}
  \vspace{12pt}

  \noindent
  \vspace{12pt}
  \textbf{Partial Derivatives}\\
  \begin{tabular}{l}
    \vspace{12pt}
    $I_{t}(x,y,t) = \frac{I(x,y,t+\Delta t) - I(x,y,t)}{\Delta t}$\\
    \vspace{12pt}
    $I_{x}(x,y,t) = \frac{I(x+\Delta x,y,t)}{2\Delta x} - \frac{I(x-\Delta x,y,t)}{2\Delta x}$\\
    \vspace{12pt}
    $I_{y}(x,y,t) = \frac{I(x,y+\Delta y,t) - I(x,y-\Delta y,t)}{2\Delta y}$\\
    \vspace{12pt}
    $I_{xx}(x,y,t) = \frac{I(x+\Delta x,y,t) - 2I(x,y,t) + I(x-\Delta x,y,t)}{\Delta x^{2}}$\\
    \vspace{12pt}
    $I_{yy}(x,y,t) = \frac{I(x,y+\Delta y,t) - 2I(x,y,t) + I(x,y-\Delta y,t)}{\Delta y^{2}}$\\
    \vspace{12pt}
    $I_{xy}(x,y,t) = \frac{I(x+\Delta x,y+\Delta y,t) - I(x+\Delta x,y-\Delta y,t) - I(x-\Delta x,y+\Delta y,t) + I(x-\Delta x,y-\Delta y,t)}{4 \Delta x\Delta y} $ \\
  \end{tabular}
  \vspace{12pt}

  \newpage
  \noindent
  \vspace{12pt}
  \textbf{CFL Condition}\\
  \begin{tabular}{l}
    \vspace{12pt}
    $\Delta t \leq \frac{1}{4} \frac{\beta \epsilon}{\lambda} \Delta x^2$\\
  \end{tabular}
  \vspace{12pt}

  \noindent
  \vspace{12pt}
  \textbf{Parameters}\\
  \begin{tabular}{l}
    \vspace{12pt}
    $\Delta t, \Delta x, \Delta y, \beta, \epsilon, \lambda $\\
  \end{tabular}
  \vspace{12pt}

  \noindent
  \vspace{12pt}
  \textbf{Format of Implemented PDE}\\
  \begin{tabular}{l}
    \vspace{12pt}
    $I(x,y,t+\Delta t) = I(x,y,t) + \Delta t (I_{t})$\\
  \end{tabular}
  \vspace{12pt}




  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  % Experimental Procedure
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \newpage
  \section{Experimental Procedure}





  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  % Experimental Results
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \newpage
  \section{Experimental Results}


  \section{Summary}


  \begin{thebibliography}{unsrt}
    \bibitem{Thomas_book}
      Thomas, L. \& Ari, R. d. \emph{Biological Feedback} (CRC Press, USA, 1990).
  \end{thebibliography}

\end{document}