\documentclass{article}
\usepackage{amssymb}
\usepackage{amsmath}
\title{\textbf{ECE 6560 Final Project - Image Smoothing}}
\author{Alexander Domagala}
\date{Spring 2024}

\begin{document}
  \maketitle

  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  % Problem Description
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \section{Problem Description}
  Although image processing techniques began to appear around the 1960s, the proliferation of inexpensive
  digital computing power has greatly widened the application pool. Image processing techniques
  can now be found in areas such as digital photography, medical imaging, and object detection/tracking.\\

  \noindent
  All these application areas can be affected by a very common problem: image noise.
  Noise can render further processing ineffective, thus there exist a variety of approaches by which
  we can attempt to smooth and denoise our images. Traditional image processing techniques
  may rely on fixed kernels that can be swept over an image in order to smooth it. However,
  this type of approach will result in uniform smoothing can blur the edges of an image.\\

  \noindent
  We can instead leverage PDEs to describe how an image should be updated depending on its local characteristics.
  More specifically, we will examine Ansiotropic Diffusion. This technique can be used to reduce image noise
  while lessening the blurring that is done to edges. This project will attempt to develop PDEs that
  can be used to reduce noise in high/low contrast images.\\

  \noindent
  We need more information about the actual high/low contrast problem!\\


  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  % Mathematical Modeling
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \newpage
  \section{Mathematical Modeling}
  Before explicitly developing our PDEs, we must first understand what behavior we want our system to have.
  The calculus of variations can be used to minimize an energy functional. The choice of setup for the
  energy functional will determine the system behavior.
  \\

  \noindent
  Let's begin by defining our image as
    \begin{center}
      $I(x,y)$
    \end{center}
  Note that the derivations in sections (2) and (3) are being performed
  exclusively in continuous space: $x,y \in \mathbb{R}$.\\

    

  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  % Derivation of PDE
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \newpage
  \section{Derivation of PDE}

    \noindent Now, let's introduce the Euler-Lagrange equation
      \begin{center}
        \begin{tabular}{l}
          $L_{f} - \frac{\partial}{\partial x}L_{f'} = 0$ (1-D)\\
          $L_{I} - \frac{\partial}{\partial x}L_{I_{x}} - \frac{\partial}{\partial y}L_{I_{y}} = 0$ (2-D)\\
        \end{tabular}
      \end{center}
    \vspace{12pt}

    \noindent
    We can begin working towards obtaining our PDE by setting up a gradient descent
      \begin{center}
        \begin{tabular}{l}
          $I_{t} = -\nabla_{I}E$\\
          $I_{t} = -L_{I} + \frac{\partial}{\partial x}L_{I_{x}} + \frac{\partial}{\partial y}L_{I_{y}}$
        \end{tabular}
      \end{center}
    \vspace{12pt}

    \noindent
    We will now have to compute terms $L_{I}$, $L_{I_{x}}$, $L_{I_{y}}$ using the previously obtained energy functional
      \begin{center}
        \begin{tabular}{l}
          $L(I,I_{x},I_{y},x,y) = \frac{\lambda}{1+e^{\alpha}}$, where $\alpha = -\frac{1}{\beta}(\| \nabla_{I} \|)$\\
          We will also include a term $\epsilon$ such that $\| \nabla_{I} \| = \sqrt{I_{x}^2 + I_{y}^2 + \epsilon^2}$\\
          $\epsilon$ is a constant used to prevent stability issues and will be examined more\\
          \vspace{12pt}
          closely in section (4).\\
          $L_{I} = \frac{\partial}{\partial I}(L)$\\
          $L_{I} = 0$\\
          $L_{I_{x}} = \frac{\partial}{\partial I_{x}}(L)$\\
          $L_{I_{x}} = \frac{\lambda}{\beta} \frac{e^\alpha}{(1+e^{\alpha})^2} \frac{I_{x}}{\sqrt{I_{x}^2 + I_{y}^2 + \epsilon^2}}$\\
          $L_{I_{y}} = \frac{\partial}{\partial I_{y}}(L)$\\
          $L_{I_{y}} = \frac{\lambda}{\beta} \frac{e^\alpha}{(1+e^{\alpha})^2} \frac{I_{y}}{\sqrt{I_{x}^2 + I_{y}^2 + \epsilon^2}}$\\
        \end{tabular}
      \end{center}
      \vspace{12pt}

    \noindent
    Now that we have obtained our expressions for $L_{I_{x}}$ and $L_{I_{y}}$, we must compute their partial derivatives
    as shown by the Euler-Lagrange equation. This will be shown for $\frac{\partial}{\partial x}L_{I_{x}}$. $\frac{\partial}{\partial y}L_{I_{y}}$ will be obtained by examining
    the expression of $\frac{\partial}{\partial x}L_{I_{x}}$.\\

    \noindent
    Let $\phi$ denote $\frac{e^\alpha}{(1+e^{\alpha})^2}$ and let $\gamma$ denote $\frac{I_{x}}{\sqrt{I_{x}^2 + I_{y}^2 + \epsilon^2}}$.
    We can begin finding $\frac{\partial}{\partial x}L_{I_{x}}$ by using the product-rule $\frac{\partial}{\partial x}(\gamma)\phi + \frac{\partial}{\partial x}(\phi)\gamma$.
    We will start with the left side of the sum. Note that we must include $\frac{\lambda}{\beta}$ in the final expression.\\
    \begin{center}
      \begin{tabular}{l}
        \vspace{12pt}
        $\frac{\partial}{\partial x}(\gamma)\phi$\\
        \vspace{12pt}
        $\frac{\partial}{\partial x}(\frac{I_{x}}{\sqrt{I_{x}^2 + I_{y}^2 + \epsilon^2}})\phi$\\
        $(\frac{I_{xx}}{(I_{x}^2 + I_{y}^2 + \epsilon^2)^\frac{1}{2}} + \frac{I_{x}}{(I_{x}^2 + I_{y}^2 + \epsilon^2)^\frac{3}{2}}(I_{x}I_{xx} + I_{y}I_{xy}))\phi$
      \end{tabular}
    \end{center}
    \vspace{12pt}
    
    \newpage
    \noindent
    We can now examine the right side of $\frac{\partial}{\partial x}(\gamma)\phi + \frac{\partial}{\partial x}(\phi)\gamma$.
    \begin{center}
      \begin{tabular}{l}
        $\frac{\partial}{\partial x}(\phi)\gamma$\\
        $\frac{\partial}{\partial x}(\frac{e^\alpha}{(1+e^{\alpha})^2})\gamma$\\
        $\frac{\partial}{\partial x}((e^\alpha)(1+e^{\alpha})^{-2})\gamma$\\
      \end{tabular}
    \end{center}

    \noindent
    We see that we will need to again perform the product-rule between $(e^\alpha)$ and\\
    $(1+e^{\alpha})^{-2}$. Taking the partial derivative of $(e^\alpha)$
    \begin{center}
      \begin{tabular}{l}
        $-\frac{1}{\beta} e^{\alpha} \frac{1}{(I_{x}^2 + I_{y}^2 + \epsilon^2)^\frac{1}{2}} (I_{x}I_{xx}+I_{y}I_{xy})$
      \end{tabular}
    \end{center}

    \noindent
    Taking the partial derivative of $(1+e^{\alpha})^{-2}$
    \begin{center}
      \begin{tabular}{l}
        $-2(1+e^{\alpha})^{-3} (-\frac{1}{\beta} e^{\alpha} \frac{1}{(I_{x}^2 + I_{y}^2 + \epsilon^2)^\frac{1}{2}} (I_{x}I_{xx}+I_{y}I_{xy})) $
      \end{tabular}
    \end{center}

    \noindent
    Thus, after factoring common terms, $\frac{\partial}{\partial x}((e^\alpha)(1+e^{\alpha})^{-2})$ yields
    \begin{center}
      \begin{tabular}{l}
        $[-\frac{1}{\beta} (e^\alpha) (\frac{1}{(I_{x}^2 + I_{y}^2 + \epsilon^2)^\frac{1}{2}}) (I_{x}I_{xx}+I_{y}I_{xy})] [(1+e^{\alpha})^{-2} + (e^\alpha)(-2(1+e^{\alpha})^{-3})]$\\
      \end{tabular}
    \end{center}
    \vspace{12pt}
    \vspace{12pt}

  \noindent
  We have reached the final expression for $\frac{\partial}{\partial x}L_{I_{x}}$
  \begin{center}
    \begin{tabular}{l}
      \vspace{12pt}
      $\frac{\partial}{\partial x}L_{I_{x}} =$\\
      \vspace{12pt}
      $\frac{\lambda}{\beta}[ (\frac{I_{xx}}{(I_{x}^2 + I_{y}^2 + \epsilon^2)^\frac{1}{2}}) - (\frac{I_{x}}{(I_{x}^2 + I_{y}^2 + \epsilon^2)^\frac{3}{2}}) (I_{x}I_{xx} + I_{y}I_{xy}) (\frac{e^\alpha}{(1+e^{\alpha})^2}) +$\\
      \vspace{12pt}
      $(\frac{I_{x}}{(I_{x}^2 + I_{y}^2 + \epsilon^2)^\frac{1}{2}}) [-\frac{1}{\beta} (e^\alpha) (\frac{1}{(I_{x}^2 + I_{y}^2 + \epsilon^2)^\frac{1}{2}}) (I_{x}I_{xx}+I_{y}I_{xy})] [(1+e^{\alpha})^{-2} + (e^\alpha)(-2(1+e^{\alpha})^{-3})]]$
    \end{tabular}
  \end{center}
  \vspace{12pt}

  \noindent
    $\frac{\partial}{\partial y}L_{I_{y}}$ can be obtained by modifying $\frac{\partial}{\partial x}L_{I_{x}}$
    \begin{center}
      \begin{tabular}{l}
        \vspace{12pt}
        $\frac{\partial}{\partial y}L_{I_{y}} =$\\
        \vspace{12pt}
        $\frac{\lambda}{\beta}[ (\frac{I_{yy}}{(I_{x}^2 + I_{y}^2 + \epsilon^2)^\frac{1}{2}}) - (\frac{I_{y}}{(I_{x}^2 + I_{y}^2 + \epsilon^2)^\frac{3}{2}}) (I_{x}I_{xy} + I_{y}I_{yy}) (\frac{e^\alpha}{(1+e^{\alpha})^2}) +$\\
        \vspace{12pt}
        $(\frac{I_{y}}{(I_{x}^2 + I_{y}^2 + \epsilon^2)^\frac{1}{2}}) [-\frac{1}{\beta} (e^\alpha) (\frac{1}{(I_{x}^2 + I_{y}^2 + \epsilon^2)^\frac{1}{2}}) (I_{x}I_{xy}+I_{y}I_{yy})] [(1+e^{\alpha})^{-2} + (e^\alpha)(-2(1+e^{\alpha})^{-3})]]$
      \end{tabular}
    \end{center}
    \vspace{12pt}

    \newpage
    \noindent
    Our final gradient-descent PDE is
    \begin{center}
      \begin{tabular}{l}
        \vspace{12pt}
        $I_{t} = \frac{\lambda}{\beta}[ (\frac{I_{xx}}{(I_{x}^2 + I_{y}^2 + \epsilon^2)^\frac{1}{2}}) - (\frac{I_{x}}{(I_{x}^2 + I_{y}^2 + \epsilon^2)^\frac{3}{2}}) (I_{x}I_{xx} + I_{y}I_{xy}) (\frac{e^\alpha}{(1+e^{\alpha})^2}) +$\\
        \vspace{12pt}
        $(\frac{I_{x}}{(I_{x}^2 + I_{y}^2 + \epsilon^2)^\frac{1}{2}}) [-\frac{1}{\beta} (e^\alpha) (\frac{1}{(I_{x}^2 + I_{y}^2 + \epsilon^2)^\frac{1}{2}}) (I_{x}I_{xx}+I_{y}I_{xy})] [(1+e^{\alpha})^{-2} + (e^\alpha)(-2(1+e^{\alpha})^{-3})]+ $\\
        \vspace{12pt}
        $(\frac{I_{yy}}{(I_{x}^2 + I_{y}^2 + \epsilon^2)^\frac{1}{2}}) - (\frac{I_{y}}{(I_{x}^2 + I_{y}^2 + \epsilon^2)^\frac{3}{2}}) (I_{x}I_{xy} + I_{y}I_{yy}) (\frac{e^\alpha}{(1+e^{\alpha})^2}) +$\\
        \vspace{12pt}
        $(\frac{I_{y}}{(I_{x}^2 + I_{y}^2 + \epsilon^2)^\frac{1}{2}}) [-\frac{1}{\beta} (e^\alpha) (\frac{1}{(I_{x}^2 + I_{y}^2 + \epsilon^2)^\frac{1}{2}}) (I_{x}I_{xy}+I_{y}I_{yy})] [(1+e^{\alpha})^{-2} + (e^\alpha)(-2(1+e^{\alpha})^{-3})]]$
        \vspace{12pt}
      \end{tabular}
    \end{center}

    \noindent
    Where $\alpha = -\frac{1}{\beta}(\| \nabla_{I} \|)$ and $\| \nabla_{I} \| = \sqrt{I_{x}^2 + I_{y}^2 + \epsilon^2}$\\



  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  % Discretization and Implementation
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \newpage
  \section{Discretization and Implementation}
  \noindent
  The PDE that was obtained in the previous section is only applicable for continuous
  time and space variables. We must discretize the PDE so that it can actually be implemented in software.\\

  \noindent
  Before explicitly discretizing the PDE, we must also obtain numeric approximations for its partial
  derivatives. Beginning with the left side of the PDE, let's approximate $I_{t}$. 
  It is important to understand that the PDE is defining how the image will be smoothed
  over successive iterations. In other words, it specifies how the image is updated as time increases.
  Thus, it is appropriate to use a forward difference.\\

  \noindent
  The forward difference for a single (space) variable can be obtained using the 
  numeric differentiation method shown in class
  \begin{center}
    \begin{tabular}{l}
      \vspace{12pt}
      $f(x,t) = f(x,t)$\\
      $f(x,t+\Delta t) = f(x,t) + \Delta t f'(x,t) + O(\Delta t^2)$\\
    \end{tabular}
  \end{center}

  \noindent
  After eliminating $f(x,t)$ terms on the right side of the above equations,
  we obtain our approximation. Note that the approximation includes an error term $O(\Delta t)$.
  \begin{center}
    \begin{tabular}{l}
      $f'(x,t) = \frac{f(x,t+\Delta t) - f(x,t)}{\Delta t}$\\
    \end{tabular}
  \end{center}

  \noindent
  Expanding this process to two (space) dimensions yields
  \begin{center}
    \begin{tabular}{l}
      $I_{t}(x,y,t) = \frac{I(x,y,t+\Delta t) - I(x,y,t)}{\Delta t}$\\
    \end{tabular}
  \end{center}
  \vspace{12pt}

  \noindent
  Continuing with the right side of the PDE, approximations are needed for the following partial
  derivatives: $I_{x}$, $I_{y}$, $I_{xx}$, $I_{yy}$, and $I_{xy}$. Since these approximations
  capture how the image is changing with respect to space, it is more appropriate to use a
  central difference rather than a forward difference.\\
  
  \noindent
  Suppose we are trying to approximate
  $I_{x}$. A forward difference would introduce some bias because the value of $I_{x}$ at
  a certain point is dependent on an adjacent image value in the positive x-direction only.
  The central difference will balance out the value of $I_{x}$ since both directions
  are considered. The intent is to make the approximation more robust to variations in image content. \\

  \newpage
  \noindent
  The central difference for a single (space) variable can be obtained using the
  numeric differentiation method shown in class
  \begin{center}
    \begin{tabular}{l}
      \vspace{12pt}
      $f(x,t) = f(x,t)$\\
      \vspace{12pt}
      $f(x+\Delta x,t) = f(x,t) + \Delta x f'(x,t) + O(\Delta x^2)$\\
      \vspace{12pt}
      $f(x-\Delta x,t) = f(x,t) - \Delta x f'(x,t) + O(\Delta x^2)$\\
    \end{tabular}
  \end{center}

  \noindent
  After eliminating $f(x,t)$ terms on the right side of the above equations,
  we obtain our approximation. Note that the approximation includes an error term $O(\Delta x)$.
  \begin{center}
    \begin{tabular}{l}
      \vspace{12pt}
      $f'(x,t) = \frac{f(x+\Delta x,t) - f(x-\Delta x,t)}{2\Delta x}$\\
    \end{tabular}
  \end{center}

  \noindent
  Expanding this process to two (space) dimensions yields
  \begin{center}
    \begin{tabular}{l}
      \vspace{12pt}
      $I_{x}(x,y,t) = \frac{I(x+\Delta x,y,t) - I(x-\Delta x,y,t)}{2\Delta x}$\\
      \vspace{12pt}
      $I_{y}(x,y,t) = \frac{I(x,y+\Delta y,t) - I(x,y-\Delta y,t)}{2\Delta y}$\\
    \end{tabular}
  \end{center}

  \noindent
  In order to obtain approximations for $I_{xx}$ and $I_{yy}$, we can simply add additional
  terms to the Taylor Series expansion and then repeat the elimination process
  \begin{center}
    \begin{tabular}{l}
      \vspace{12pt}
      $I_{xx}(x,y,t) = \frac{I(x+\Delta x,y,t) - 2I(x,y,t) + I(x-\Delta x,y,t)}{\Delta x^{2}}$\\
      \vspace{12pt}
      $I_{yy}(x,y,t) = \frac{I(x,y+\Delta y,t) - 2I(x,y,t) + I(x,y-\Delta y,t)}{\Delta y^{2}}$\\
    \end{tabular}
  \end{center}
  \vspace{12pt}

  \noindent
  Finally, we need to obtain an approximation for the mixed partial derivative $I_{xy}$.
  This is achieved by taking the central difference approximation for $I_{x}$ and 
  applying another central difference with respect to $y$
  \begin{center}
    \begin{tabular}{l}
      \vspace{12pt}
      $I_{x}(x,y,t) = \frac{I(x+\Delta x,y,t)}{2\Delta x} - \frac{I(x-\Delta x,y,t)}{2\Delta x}$\\
      \vspace{12pt}
      $I_{xy}(x,y,t) = \frac{\partial}{\partial y}(\frac{I(x+\Delta x,y,t)}{2\Delta x}) - \frac{\partial}{\partial y}(\frac{I(x-\Delta x,y,t)}{2\Delta x})$\\
      \vspace{12pt}
      $I_{xy}(x,y,t) = \frac{1}{2\Delta y}(\frac{I(x+\Delta x,y+\Delta y,t)}{2\Delta x} - \frac{I(x+\Delta x,y-\Delta y,t)}{2\Delta x}) - \frac{1}{2\Delta y}(\frac{I(x-\Delta x,y+\Delta y,t)}{2\Delta x} - \frac{I(x-\Delta x,y-\Delta y,t)}{2\Delta x})$\\
      \vspace{12pt}
      $I_{xy}(x,y,t) = \frac{I(x+\Delta x,y+\Delta y,t) - I(x+\Delta x,y-\Delta y,t) - I(x-\Delta x,y+\Delta y,t) + I(x-\Delta x,y-\Delta y,t)}{4 \Delta x\Delta y} $ \\
    \end{tabular}
  \end{center}

  \newpage
  \noindent
  Now that we have obtained all the necessary numeric approximations of the partial derivatives, we
  can move towards defining a scheme that discretizes the PDE. This involves selecting
  some important parameters.\\

  \noindent
  First, we will examine what is an appropriate time-step $\Delta t$. Previously,
  we defined $I_{t}(x,y,t) = \frac{I(x,y,t+\Delta t) - I(x,y,t)}{\Delta t}$. In our implementation,
  $\Delta t$ will multiply the right side of the PDE. The product will then be added to
  the current image to obtain the updated image. Thus, we must find a CFL condition
  so that we can find an appropriate time-step. Recall that during the derivation of
  the PDE, a term $\epsilon$ was included so that $\| \nabla_{I} \| = \sqrt{I_{x}^2 + I_{y}^2 + \epsilon^2}$.
  Suppose the gradient became very small, all terms $I_{x}$ and $I_{y}$ in the PDE would go to zero and 
  we would be left with $I_{t} = \frac{\lambda}{\beta}[(\frac{I_{xx}+I_{yy}}{(\epsilon^2)^\frac{1}{2}})]$.
  Then we simply have the heat equation
  \begin{center}
    \begin{tabular}{l}
      $I_{t} = \frac{\lambda}{\beta \epsilon}\Delta I$
    \end{tabular}
  \end{center}

  \noindent
  We can include our constants with the form of the CFL condition for the two-dimensional linear heat equation
  as shown in class
  \begin{center}
    \begin{tabular}{l}
      \vspace{12pt}
      $\frac{\lambda}{\beta \epsilon} \leq \frac{1}{4} \Delta x^2$\\
      \vspace{12pt}
      $\Delta t \leq \frac{1}{4} \frac{\beta \epsilon}{\lambda} \Delta x^2$\\
    \end{tabular}
  \end{center}
  \vspace{12pt}

  \noindent
  With a bound set for  $\Delta t$, we can now shift our focus to $\Delta x$ and $\Delta y$. Since
  a digital image is composed of a finite number of individual pixels, we will compute the different
  partial derivative approximations by simply taking our current pixel and applying an offset of
  +1 or -1 to the current pixel's index, depending on the partial derivative we are trying to compute.


















  \newpage
  \section{Experimental Results}


  \section{Summary}

\end{document}